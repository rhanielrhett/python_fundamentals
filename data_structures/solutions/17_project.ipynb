{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Tweets Analysis\n",
    "\n",
    "\n",
    "In this section, we will go through an example analysis of tweets about airlines. We will bring together the basic programming, loading data, and statistical analysis/ visualization techniques from Parts 1-3 of this workshop to analyze airline tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Dataset\n",
    "\n",
    "The dataset is from the [Airline tweets sentiment dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?resource=download), which contains tweets that tag one of several major airlines. The dataset also includes information about the tweet location and time, the airline mentioned, and the sentiment of the tweet.\n",
    "\n",
    "\n",
    "First, let's import the packages to use in this analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data:\n",
    "\n",
    "First let's import in our data set. The data are located in the `airline_data` subfolder of this directory. Let's see what is in that subfolder using `os.listdir()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('airline_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.1: Load in a single file \n",
    "First, let's load in a single file and take a look at it. \n",
    "\n",
    "1. Read in the `Delta.csv` file as a `pandas` object.\n",
    "2. How many rows are there? How many columns?\n",
    "3. Which columns seem most informative? Are there any extra or redundant columns? \n",
    "4. Where is airline represented in the csv file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## solution\n",
    "single_airline = pd.read_csv('airline_data/Delta.csv')\n",
    "\n",
    "single_airline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the airline column is not present in any column, but is in the title of the csv file. Let's extract that information and add it to the dataframe in a column called `airline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solution\n",
    "filename = 'airline_data\\Delta.csv'.split('\\\\')[1]\n",
    "print(filename)\n",
    "name = filename.split('.')[0]\n",
    "print(name)\n",
    "single_airline['airline'] = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function `process_file(filepath)` that loads in a file with a filepath and returns the dataframe with the airline column added. Most of the steps should be already done in the cell above, now we just need to add it into the function wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solution\n",
    "def process_file(filepath):\n",
    "    df= pd.read_csv(filepath)\n",
    "    df['airline'] = filepath.split('\\\\')[1].split('.')[0].lower()\n",
    "    \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another filepath: `'data/US-Airways.csv'` What will be in the airline column in the output? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file('airline_data\\\\US-Airways.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we may want to modify our `process_file` function to make sure that multi-word airlines have a space rather than a hyphen between words, but for now we will move forward in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load in multiple files\n",
    "\n",
    "Now that we have a function, let's iterate through all of files in the directory. \n",
    "\n",
    "First, fill in the blanks below to loop through and print every file in the `airline_data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## solution\n",
    "directory = 'airline_data'\n",
    "for file in os.listdir(directory):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there is a `.txt` file in the directory, which isn't a `pandas` dataframe. This will cause an error in the dataframe processing, so let's use an if statement to filter out the `.txt` extension. \n",
    "\n",
    "\n",
    "First let's write an expression that evaluates to `True` for `.csv` files and `False` for `.txt` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv='delta.csv' #expression should evaluate True\n",
    "test_txt='delta.txt' # evaluate false\n",
    "\n",
    "test_csv.endswith('csv')\n",
    "test_txt.endswith('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## solution\n",
    "directory = 'airline_data'\n",
    "for file in os.listdir(directory):\n",
    "    if file.split('.')[1]=='csv':\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all of the right files to process. Let's build a script that processes each file using the function from above and accumulates it into a list of dataframes. As the first step, I substitute in the `process_file` function from above. That results in the error below. What is the error? How might we resolve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'airline_data'\n",
    "for file in os.listdir(directory):\n",
    "    if file.split('.')[1]=='csv':\n",
    "        process_file(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up the function `os.path.join()` (and recall our File i/o notebook). How can this help dynamically make the filepath? What might be the advantage of this method over string concatenation? \n",
    "\n",
    "Update our loop to resolve the error from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solution\n",
    "\n",
    "directory = 'airline_data'\n",
    "for file in os.listdir(directory):\n",
    "    if file.split('.')[1]=='csv':\n",
    "        process_file(os.path.join(directory,file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to take all of the airline DataFrames and put them together into a single `DataFrame`. \n",
    "\n",
    "*Without coding* (for now), write out the steps for the file processing code, including the aggregation steps described above. The first couple of steps are filled out for you:\n",
    "\n",
    "1. Get a list of files in a directory\n",
    "2. For each file in the directory\n",
    "    1. If the file ends in csv\n",
    "    2. Join the file and directory name and process file\n",
    "    3. Append the dataframe to a list\n",
    "3. Concatenate the list into a single dataframe\n",
    "\n",
    "Now let's use these steps to aggregate these into a list and concatenate them into a whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solution\n",
    "\n",
    "dflist = []\n",
    "directory = 'airline_data'\n",
    "for file in os.listdir(directory):\n",
    "    if file.split('.')[1]=='csv':\n",
    "        full_path = os.path.join(directory,file)\n",
    "        print(full_path)\n",
    "        dflist.append(process_file(full_path))\n",
    "        \n",
    "df = pd.concat(dflist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the final data frame.\n",
    "\n",
    "1. How many rows and columns are there in the total dataframe?\n",
    "2. How many unique airlines are in the dataset?\n",
    "3. How many numeric columns are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solution\n",
    "df.shape\n",
    "\n",
    "df['airline'].nunique()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing\n",
    "\n",
    "\n",
    "## 2.1 Nulls\n",
    "\n",
    "First, let's summarize the null values in the dataset. First, let's look at which columns have null values in them. Which columns have null values? What are some ways that we could deal with them?\n",
    "\n",
    "**Hint**: `pd.isnull()` may be a good place to start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##solution\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be using any of the columns with null values in the analysis, so we don't need to drop any rows from this dataset. \n",
    "\n",
    "Let's drop the columns: `tweet_id`, `airline_sentiment_confidence`,`negativereason_confidence`,\n",
    "`airline_sentiment_gold`,`airline_sentiment_gold`,`tweet_coord`,\n",
    "`tweet_location`,`user_timezone`\n",
    "\n",
    "This will make the dataset more manageable for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##solution\n",
    "columns_to_drop = ['Unnamed: 0','tweet_coord','tweet_id','user_timezone',\n",
    "         'tweet_created','tweet_location','negativereason_gold',\n",
    "        'airline_sentiment_gold']\n",
    "df.drop(columns_to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature extraction\n",
    "\n",
    "Now let's do some basic preprocessing on the data. First, let's look at the first few rows of the dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a couple of simple feature extraction on the text data, including the number of words. Let's make three new columns:\n",
    "1. `word_count`: number of words in each tweet\n",
    "2. `mentions` : count number of '@' symbols\n",
    "3. one other text feature (your choice): for example number of capital words, links, or punctuation like '!', '?', etc. \n",
    "\n",
    "\n",
    "**Hint:** Remember that you can use `Series.str` to access vectorized string functions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['word_count'] = df['text'].str.split(' ').str.len()\n",
    "\n",
    "df['mentions'] = df['text'].str.count('@')\n",
    "\n",
    "\n",
    "# final one your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps in text preprocessing would often use tokenization or vectorization on tweets, to convert the words themselves to numerical data for preprocessing. If you are interested, check out the Python Text Analysis workshop! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Subset Tweets\n",
    "\n",
    "**Question:** How many sentiment types are there in the DataFrame? \n",
    "\n",
    "For our exploratory analysis, let's start by looking just at postive/negative tweets.\n",
    "\n",
    "1. Subset the dataframe\n",
    "2. What proportion of the tweets have a positive sentiment?\n",
    "\n",
    "What is the condition that we would use to subset the dataframe? Subset the dataframe for non-neutral tweets and save it to a dataframe called `pos_neg_df`.\n",
    "**Hint:** You can use `!=` to check for all values not equal to a certain value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##solution\n",
    "\n",
    "df['airline_sentiment'].unique()\n",
    "pos_neg_df = df.loc[(df['airline_sentiment']!= 'neutral'),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `airline_sentiment` column has the terms `positive` and `negative` in it. Let's change them to a numerical column, where 1 = positive, and 0 = negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Exploratory analysis\n",
    "\n",
    "##  3.1 Most common users, most frequent airlines\n",
    "\n",
    "Let's look at the users tweeting at the airlines. \n",
    "\n",
    "1. How many unique users are there in the dataset? \n",
    "2. Who tweeted the most about airlines in this dataset? (**Hint**: consider df.value_counts())\n",
    "3. Choose one of the users with the top five most tweets. Which airline are they tweeting about?\n",
    "\n",
    "**Hint**: Users are recorded in the `name` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## solution\n",
    "\n",
    "pos_neg_df['name'].unique()\n",
    "users = pos_neg_df.value_counts('name')\n",
    "pos_neg_df.loc[pos_neg_df['name'] == 'otisday','airline'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format doesn't give a great idea of the overall distribution of the data. Let's plot this data in a histogram using `pd.plot`. How would I add a title and x-axis label to the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## solution\n",
    "df['name'].value_counts().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##solution\n",
    "res = sm.stats.ttest_ind(pos_neg_df.loc[pos_neg_df['airline_sentiment']==0,'word_count'],\n",
    "                         pos_neg_df.loc[pos_neg_df['airline_sentiment']==1,'word_count'])\n",
    "res\n",
    "\n",
    "pos_neg_df.loc[pos_neg_df['airline_sentiment']==0,'word_count'].plot(kind='hist')\n",
    "pos_neg_df.loc[pos_neg_df['airline_sentiment']==1,'word_count'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Linear Regression of Tweet Length\n",
    "\n",
    "Now that we undersdand Let's use a linear regression to look at other predictors of tweet length. Complete the steps:\n",
    "\n",
    "1. Select the numeric columns 'airline_sentiment','airline_sentiment_confidence','retweet_count','hashtags','mentions', and save it as `X` (except wordcount)\n",
    "2. Select the word_count column and save as `y`\n",
    "3. Set up a linear regression and fit it to the data using `sm.OLS()`\n",
    "4. Interpret the model summary\n",
    "\n",
    "**Bonus**: How many lines of code did it take? Can you shorten it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## solution\n",
    "X = np.array(pos_neg_df[['airline_sentiment','airline_sentiment_confidence','retweet_count','hashtags','mentions']],dtype=float)\n",
    "\n",
    "y = np.array(pos_neg_df['word_count'],dtype=float)\n",
    "\n",
    "model=sm.OLS(y,X).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Are negative tweets longer than positive tweets?\n",
    "\n",
    "Let's take a look at the negative and positive tweets. We are interested in the question of whether negative tweets are longer than positive tweets. Let's test this with a t-test.\n",
    "\n",
    "1. Subset the data into positive and negative tweets\n",
    "2. Select the `word_count` column\n",
    "3. Calculate the mean word count for each column. Which mean is higher?\n",
    "3. Use a t-test to compare the two sets of values from (2). What is the p-value of the result? \n",
    "4. Plot a histogram layer for both positive and negative tweet word counts. Choose an appropriate value for `bins`. What do you notice about the distribution?\n",
    "\n",
    "**Hint**: Refer to the statsmodels notebook from Day 3 for an example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a linear regression to look at other predictors of tweet length. \n",
    "Steps:\n",
    "1. Select the numeric columns and save it as `X` from the dataframe (except wordcount)\n",
    "2. Select word_count column and save as `y`\n",
    "3. Set up a linear regression and fit it to the data\n",
    "4. Interpret the model summary\n",
    "\n",
    "\n",
    "**Bonus**: How many lines of code did it take? Can you shorten it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook took us through importing multiple csv files, data manipulation, and some basic visualizations and analysis of data. If you were working on this dataset, what would you potentially do next? It could be either an analysis, a new feature to include, a visualization that might help represent the data, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
