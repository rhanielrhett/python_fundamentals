{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Tweets Analysis\n",
    "\n",
    "In this section, we will go through an example analysis of tweets about airlines. We will bring together the basic programming, loading data, and statistical analysis/ visualization techniques from Parts 1-3 of this workshop to analyze airline tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Dataset\n",
    "\n",
    "The dataset is from the [Airline tweets sentiment dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?resource=download), which contains tweets that tag one of several major airlines. The dataset also includes information about the tweet location and time, the airline mentioned, and the sentiment of the tweet.\n",
    "\n",
    "First, let's import the packages to use in this analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import Data\n",
    "\n",
    "First let's import in our data set. The data are located in the `airline_data` subfolder of this directory. Let's see what is in that subfolder using `os.listdir()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load in a Single File\n",
    "\n",
    "First, let's load in a single file and take a look at it. \n",
    "\n",
    "1. Read in the `Delta.csv` file as a `pandas` object.\n",
    "2. How many rows are there? How many columns?\n",
    "3. Which columns seem most informative? Are there any extra or redundant columns? \n",
    "4. Where is airline represented in the csv file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in file for Delta\n",
    "single_airline = pd.read_csv(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the airline column is not present in any column, but is in the title of the csv file. Let's extract that information and add it to the DataFrame in a column called `airline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract filename from path\n",
    "filename = 'airline_data/Delta.csv'.split('/')[1] \n",
    "print(filename)\n",
    "# Fill in the blanks\n",
    "name = filename.split(___)[___]\n",
    "\n",
    "single_airline['airline'] = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function `process_file(filepath)` that loads in a file with a filepath and returns the dataframe with the airline column added. Most of the steps should be already done in the cell above, now we just need to add it into the function wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Add code to extract airline name and save it to a name variable\n",
    "    name = ___\n",
    "    df['airline'] = name\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another filepath: `'data/US-Airways.csv'` What will be in the airline column in the output from the function below?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file('data/US-Airways.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we may want to modify our `process_file()` function to make sure that multi-word airlines have a space rather than a hyphen between words, but for now we will move forward in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load in Multiple Files\n",
    "\n",
    "Now that we have a function, let's iterate through all of files in the directory. \n",
    "\n",
    "First, fill in the blanks below to loop through and print every file in the `airline_data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'airline_data'\n",
    "for file in os.____(____):\n",
    "    print(______)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there is a `.txt` file in the directory, which isn't a `pandas` dataframe. This will cause an error in the dataframe processing, so let's use an if statement to filter out the `.txt` extension. \n",
    "\n",
    "\n",
    "First let's write an expression that evaluates to `True` for `.csv` files and `False` for `.txt` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv='delta.csv' # Expression should evaluate True\n",
    "test_txt='delta.txt' # Evaluate false\n",
    "\n",
    "# Test both files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an expression, let's add that into our for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "directory = 'airline_data'\n",
    "for file in os.listdir(directory):\n",
    "    if _____ # Fill in the blank to filter for files ending with `.csv`\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What cases will you solution *not* work for? Can you think of any edge cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a loop that is going through all of the right files to process. Let's build a script that processes each file using the `process_file` function from above and *accumulates* it into a list of dataframes. \n",
    "\n",
    "1. substitute in the `process_file` function from above for the print line for the code from the previous cell\n",
    "2. Run the code. This results in the error below. What is the error type? What might be causing it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = 'airline_data'\n",
    "for file in os.listdir(directory):\n",
    "    if _____: # Fill in the blank to filter for files ending with `.csv`\n",
    "        process_file(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some research! Once we've formulated what went wrong, we can try to formulate a strategy. \n",
    "\n",
    "*Without writing code* (for now) find a link to documentation or blog post for a function you can use to help. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a plan, we can choose the best strategy for this situation! Let's update the code from the cell above to address the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to take all of the airline DataFrames and put them together into a single `DataFrame`. \n",
    "\n",
    "*Without coding* (for now), write out the steps for the file processing code, including the aggregation steps described above. The first couple of steps are filled out for you:\n",
    "\n",
    "1. Get a list of files in a directory\n",
    "2. For each file in the directory\n",
    "    1. ...continued\n",
    "\n",
    "Now let's use these steps to aggregate these into a list and concatenate them into a whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "dflist = []\n",
    "directory = 'airline_data'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "df = ___.____(dflist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the final data frame.\n",
    "\n",
    "1. How many rows and columns are there in the total dataframe?\n",
    "2. How many unique airlines are in the dataset?\n",
    "3. How many numeric columns are there in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Processing\n",
    "\n",
    "Now that we have some data, let's take a look at some data processing steps\n",
    "\n",
    "### 2.1 Nulls\n",
    "\n",
    "First, let's summarize the null values in the dataset. We want to see which columns have null values and how many. \n",
    "\n",
    "You recall that `.isnull()` is a method that returns `True` where there are null values and a `False` otherwise in a DataFrame. \n",
    "\n",
    "You look up finding the sum of null values in Pandas and find a suggestion to use `df.isnull().sum(axis=1)`. You try this out on your data set and get the output below. Is this the expected output? If not, how can you modify the code to find the number of null values in each column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are null values in the data set, We won't be using any of the columns with null values in the analysis, so we don't need to drop any rows from this dataset. \n",
    "\n",
    "Let's drop the following columns:\n",
    "\n",
    "* `tweet_id`\n",
    "* `airline_sentiment_confidence`\n",
    "* `negativereason_confidence`\n",
    "* `airline_sentiment_gold`\n",
    "* `airline_sentiment_gold`\n",
    "* `tweet_coord`\n",
    "* `tweet_location`\n",
    "* `user_timezone`\n",
    "\n",
    "This will make the dataset more manageable for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'Unnamed: 0',\n",
    "    'tweet_coord',\n",
    "    'tweet_id',\n",
    "    'user_timezone',\n",
    "    'tweet_created',\n",
    "    'tweet_location',\n",
    "    'negativereason_gold',\n",
    "    'airline_sentiment_gold']\n",
    "list(df)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Extraction\n",
    "\n",
    "Now let's do some basic preprocessing on the data. First, let's look at the first few rows of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a couple of simple feature extraction on the text data, including the number of words. Let's make three new columns:\n",
    "1. `word_count`: number of words in each tweet\n",
    "2. `mentions` : count number of '@' symbols\n",
    "3. one other text feature (your choice): for example number of capital words, links, or punctuation like '!', '?', etc. \n",
    "\n",
    "\n",
    "**Hint:** Remember that you can use `Series.str` to access vectorized string functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df['word_count'] = ...\n",
    "df['mentions'] = ...\n",
    "\n",
    "# Final one of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps in text preprocessing would often use tokenization or vectorization on tweets, to convert the words themselves to numerical data for preprocessing. If you are interested, check out the Python Text Analysis workshop! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Subset Tweets\n",
    "\n",
    "**Question:** How many sentiment types are there in the DataFrame? \n",
    "\n",
    "For our exploratory analysis, let's start by looking just at postive/negative tweets.\n",
    "\n",
    "1. Subset the dataframe\n",
    "2. What proportion of the tweets have a positive sentiment?\n",
    "\n",
    "What is the condition that we would use to subset the dataframe? Subset the dataframe for non-neutral tweets and save it to a dataframe called `pos_neg_df`.\n",
    "**Hint:** You can use `!=` to check for all values not equal to a certain value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "pos_neg_df = df.loc[____]\n",
    "\n",
    "\n",
    "# Calculate the proportion positive sentiment tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Exploratory Analysis\n",
    "\n",
    "\n",
    "Now that we've done some very basic processing on the `DataFrame`, let's do some exploratory analyses on the data. \n",
    "\n",
    "###  3.1 Most Common Users, Most Frequent Airlines\n",
    "\n",
    "Let's look at the users tweeting at the airlines. Using the `DataFrame`, answer the following questions:\n",
    "\n",
    "1. How many unique users are there in the dataset? \n",
    "2. Who tweeted the most about airlines in this dataset? (**Hint**: consider `df.value_counts()`)\n",
    "3. Choose one of the users with the top five most tweets. Which airline are they tweeting about?\n",
    "\n",
    "**Note**: Users are recorded in the `name` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format doesn't give a great idea of the overall distribution of the data. Let's plot this data in a histogram using `pd.plot`. Add a title and x-axis label to this plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['name'].value_counts().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualization\n",
    "\n",
    "Now, let's visualize some component of the data set. Use a **histogram** to visualize the `word_counts` column. Consider plotting two layers: one for negative tweets and one for positive tweets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Linear Regression of Tweet Length\n",
    "\n",
    "Now that we undersdand Let's use a linear regression to look at other predictors of tweet length. Complete the steps:\n",
    "\n",
    "1. Select the numeric columns 'airline_sentiment','airline_sentiment_confidence','retweet_count','hashtags','mentions', and save it as `X` (except wordcount)\n",
    "2. Select the word_count column and save as `y`\n",
    "3. Set up a linear regression and fit it to the data using `sm.OLS()`\n",
    "4. Interpret the model summary\n",
    "\n",
    "**Bonus**: How many lines of code did it take? Can you shorten it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X = ...\n",
    "y = ...\n",
    "model = ...\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Are Negative Tweets Longer than Positive Tweets?\n",
    "\n",
    "Let's take a look at the negative and positive tweets. We are interested in the whether negative tweets are longer than positive tweets. Let's test this with a t-test.\n",
    "\n",
    "1. Subset the data into positive and negative tweets\n",
    "2. Select the `word_count` column\n",
    "3. Calculate the mean word count for each column. Which mean is higher?\n",
    "3. Use a t-test to compare the two sets of values from (2). What is the p-value of the result? \n",
    "4. Plot a histogram layer for both positive and negative tweet word counts. What do you notice about the distribution?\n",
    "\n",
    "**Hint**: Refer to the `statsmodels` notebook from Day 3 for an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframe\n",
    "\n",
    "# Run t-test\n",
    "res = sm.stats.ttest_ind(...)\n",
    "\n",
    "\n",
    "# Plot (kind = 'hist')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook took us through importing multiple csv files, data manipulation, and some basic visualizations and analysis of data. If you were working on this dataset, what would you potentially do next? It could be either an analysis, a new feature to include, a visualization that might help represent the data, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
